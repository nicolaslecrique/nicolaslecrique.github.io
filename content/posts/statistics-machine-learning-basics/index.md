---
title: 'Statistics and Machine Learning, the basics'
date: 2022-07-15T10:48:07+02:00
draft: true
math: true
images: []
description: null
resources:
- name: "featured-image"
  src: "featured-image.jpg"
---

The objective of this post is to recapitulate the basics of statistics and machine learning following a glossary form.


# Basic Maths 

## Eigenvalues, Eigenvectors and Eigendecomposition (Singular Value Decomposition)

$v$ is an eigenvector and $\lambda$ an eigenvalue for a square matrix $M$ if:

$Mv=\lambda v$

$M$ is called *diagonalizable* if $\exists D$ diagonal, $P$ invertible such a:

$M=PDP^{-1}$

In such case, $P$ column vectors are the *eigenvectors* and $D$ diagonal values are the *eigenvalues*. This is the *eigendecomposition* of $M$.

Some interesting properties:
* All symmetric matrix (i.e. $M=M^T$) is diagonalizable by an orthogonal matrix (i.e. $P^TP=PP^T=I$, and so $P^T=P^{-1}$)
* $M^n=PD^nP^{-1}$


# Statistics Principles


## Regression, classification, clustering

* __Regression__: labels are continuous
* __Classification__: labels are categories
* __Clustering__: no labels, group data into clusters

## Supervised, unsupervised, self-supervised learning

* __Supervised__: data is labeled. e.g. *regression*, *classification*
* __Unsupervised__ : data is unlabeled, it is used to find patterns in data. e.g *clustering*
* __self-supervised__: labels are in the data itself. e.g *language modeling*.

## Loss function / Cost function

A function (to minimize) that measures how well model predictions match labels. It should be *Lipschitz continuous* (bounded derivative) for *gradient descent*.

### Mean Square Error

Usual loss function for regressions. It penalizes heavily big prediction errors.

$MSE=||Y-\hat{Y}||^2=\frac{1}{n}\sum_{i=1}^n (Y_i-\hat{Y}_i)^2$

### Cross Entropy

Usual loss function for classifications. It's equivalent to maximizing the *likelihood* of the distribution.

$ CE=-\frac{1}{n}\sum_{i=1}^n  Y_i \cdot \ln{\hat{Y_i}}$

with $Y_i$ being a one-hot vector for the $i$-th sample with $1$ on the expected class

## Training set, Validation set, Test set

* __training set__: to train the model. ~75% of the data
* __validation set__: to check that you didn't *overfit* your model to the *training set*. ~20% of the data
* __test set__: to check that you didn't *overfit* your model hyper-parameters to your *validation set*. ~5% of the data.


## Bias, Variance and bias-variance trade-off

We take the hypothesis that we can decompose any output $y$ for an input $x$ as:

$y=f(x)+\epsilon$

with $f$ being the true function to approximate and $\epsilon$ a random variable (called *irreductible error*) such as $E[\epsilon]=0$ and $Var(\epsilon)=\sigma^2$

When computing *MSE* of the estimator $\hat{f}$, we get that:

$MSE=Bias(\hat{f})^2+Var(\hat{f})+\sigma^2$

with:

$Bias(\hat{f})=E[\hat{f}-f]$

$Var(\hat{f})=E[(\hat{f}-E[\hat{f}])^2]$

*Bias* is due to underfitting (lack of power of the algorithm), *Variance* is due to *overfitting*.


# Base algorithms

## Principal Component Analysis



## Least squares

In a regression problem, let $Y$ be the vector of the $n$ expected outputs and $X$ the $(n,p)$ input matrix. We suppose that the output can be approximated by a linear combination of the inputs. We are looking for the $p$-dimensional vector $\beta$ to minimize the MSE:

$||Y-X \beta||^2$

If we place ourselves in a $n$-dimensional vector space and call $V$ the $p$-dimensional subspace generated by $X$. Minimizing MSE is equivalent to finding $\beta$ such as $X\beta$ is the projection of $Y$ on $V$. That is:

$<Y-X\beta, Xb>=0, \forall b \in \mathbb{R}^p$

$(Xb)^T(Y-X\beta)=0, \forall b$

$b^TX^T(Y-X\beta)=0, \forall b$

$X^T(Y-X\beta)=0$

$X^TY=X^TX\beta$

We conclude that

$\beta=(X^TX)^{-1}X^TY$

## Logistic regression

In a binary classification problem, let $Y$ be the vector of the $n$ expected outputs, with values $0$, $1$ for respectively the first and second class, and $X$ the $(n,p)$ input matrix.  We are looking for the $p$-dimensional vector $\beta$ to minimize the cross-entropy $CE(Y,\hat{Y})$ with:

$\hat{Y}=\frac{1}{1+\exp{X \beta}}$

We can show it's a convex optimization problem (the second derivatives of CE with respect to all $\beta_j$ is positive) and can be solved with convex optimization algorithms (Newton...)


# Deep Learning


## Gradient descent

Optimization algorithm used in Deep Learning. Let $a$ be the vector of all model parameters and $L(a)$ the *loss function*. We compute $a$ iteratively using:

$
a = a - LearningRate * \nabla L(a)
$

## Back propagation

Algorithm used to compute the gradient of the loss function $\nabla L(a)$ with respect to all model parameters $a$.

It's based on *dynamic programming* and the *chain rule*


# Natural Language Processing

## metrics

### Precision and Recall

used for binary classification tasks like document retrieval.

* __Precision__: $ \frac{true\ positive}{true\ positive + false\ positive}$. 1 when only relevant items are found 

* __Recall__: $ \frac{true\ positive}{true\ positive + false\ negative}$. 1 when all relevant items are found


<!---
##  ------------- TODO ------------

* Language modeling
* Learning rate
* Learning rate schedule
* Regularization
* Cross-validation
* confusion matrix
* erreur de 1ere / 2eme espÃ¨ce
* model hyper-parameters
* overfitting
* accuracy, precision, recall
* stochastic grad desc (batch)
* intervall de confiance
* one hot vector
* TODO: parralel python / sklearn / pytorch sur tous les cas pertinants
* p-valeur
* layers
* softmax
* relu...
* batch, epochs
* F1-SCORE

## Covariance

## Supervised, unsupervised, semi-supervised

## Test d'hypothese

## interval de confiance

## Cross-validation

## overfitting

## training-set, validation-set, test-set

# Machine learning principles

# ---- Algorithm

## ---- MCO
-->