<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Statistics and Machine Learning, the basics (WIP) - Nicolas Lecrique</title><meta name=Description content><meta property="og:title" content="Statistics and Machine Learning, the basics (WIP)"><meta property="og:description" content="The objective of this post is to recapitulate the basics of statistics and machine learning following a glossary form."><meta property="og:type" content="article"><meta property="og:url" content="https://nicolaslecrique.github.io/posts/statistics-machine-learning-basics/"><meta property="og:image" content="https://nicolaslecrique.github.io/posts/statistics-machine-learning-basics/featured-image.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-12T08:48:07+02:00"><meta property="article:modified_time" content="2022-12-12T08:48:07+02:00"><meta property="og:site_name" content="Nicolas Lecrique"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nicolaslecrique.github.io/posts/statistics-machine-learning-basics/featured-image.png"><meta name=twitter:title content="Statistics and Machine Learning, the basics (WIP)"><meta name=twitter:description content="The objective of this post is to recapitulate the basics of statistics and machine learning following a glossary form."><meta name=application-name content="Nicolas Lecrique"><meta name=apple-mobile-web-app-title content="Nicolas Lecrique"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://nicolaslecrique.github.io/posts/statistics-machine-learning-basics/><link rel=prev href=https://nicolaslecrique.github.io/posts/python-cheat-sheet/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Statistics and Machine Learning, the basics (WIP)","inLanguage":"en-us","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/nicolaslecrique.github.io\/posts\/statistics-machine-learning-basics\/"},"genre":"posts","wordcount":2728,"url":"https:\/\/nicolaslecrique.github.io\/posts\/statistics-machine-learning-basics\/","datePublished":"2022-12-12T08:48:07+02:00","dateModified":"2022-12-12T08:48:07+02:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Nicolas Lecrique"},"description":""}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.css integrity=sha384-Cqd8ihRLum0CCg8rz0hYKPoLZ3uw+gES2rXQXycqnL5pgVQIflxAUDS7ZSjITLb5 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.js integrity=sha384-1Or6BdeNQb0ezrmtGeqQHFpppNd7a/gw29xeiSikBbsb44xu3uAo8c7FwbF5jhbd crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Nicolas Lecrique">Nicolas Lecrique</a></div><div class=menu><div class=menu-inner><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Nicolas Lecrique">Nicolas Lecrique</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Statistics and Machine Learning, the basics (WIP)</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Nicolas Lecrique</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2022-12-12>2022-12-12</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;2728 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;13 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#eigenvalues-eigenvectors-and-eigendecomposition-singular-value-decomposition>Eigenvalues, Eigenvectors and Eigendecomposition (Singular Value Decomposition)</a></li><li><a href=#sphering-whitening-of-a-matrix>Sphering (Whitening) of a Matrix</a></li></ul><ul><li><a href=#hypothesis-testing-and-p-value>Hypothesis Testing and P-Value</a></li><li><a href=#confidence-interval>Confidence interval</a></li></ul><ul><li><a href=#regression-classification-clustering>Regression, classification, clustering</a></li><li><a href=#supervised-unsupervised-self-supervised-learning>Supervised, unsupervised, self-supervised learning</a></li><li><a href=#loss-function--cost-function>Loss function / Cost function</a><ul><li><a href=#mean-square-error>Mean Square Error</a></li><li><a href=#cross-entropy>Cross Entropy</a></li></ul></li><li><a href=#training-set-validation-set-test-set>Training set, Validation set, Test set</a></li><li><a href=#bias-variance-and-bias-variance-trade-off>Bias, Variance and bias-variance trade-off</a></li><li><a href=#overfitting-and-underfitting>Overfitting and underfitting</a></li></ul><ul><li><a href=#principal-component-analysis>Principal Component Analysis</a></li><li><a href=#least-squares>Least squares</a></li><li><a href=#logistic-regression>Logistic regression</a></li><li><a href=#linearquadratic-discriminant-analysis-ldaqda>Linear/Quadratic Discriminant Analysis (LDA/QDA)</a><ul><li><a href=#use-lda-as-a-dimension-reduction-algorithm>Use LDA as a Dimension reduction algorithm</a></li></ul></li></ul><ul><li><a href=#back-propagation>Back propagation</a></li><li><a href=#gradient-descent>Gradient descent</a></li><li><a href=#stochastic-batch-gradient-descent>Stochastic (batch) gradient descent</a></li><li><a href=#momentum-rmsprop-and-adam>Momentum, RMSProp and Adam</a><ul><li><a href=#momentum>Momentum</a></li><li><a href=#rmsprop-root-mean-square-propagation>RMSProp (Root Mean Square Propagation)</a></li><li><a href=#adam-adaptative-moment-optimization>Adam (Adaptative Moment Optimization)</a></li></ul></li><li><a href=#hyper-parameters>Hyper parameters</a></li><li><a href=#common-layers>Common Layers</a><ul><li><a href=#sigmoid>Sigmoid</a></li><li><a href=#softmax>Softmax</a></li><li><a href=#relu>ReLU</a></li></ul></li><li><a href=#model-heuristics>Model Heuristics</a><ul><li><a href=#batch-normalization>Batch normalization</a></li><li><a href=#dropout-model-regularization>Dropout (model, regularization)</a></li><li><a href=#residual-connection>Residual connection</a></li></ul></li><li><a href=#data-heuristics>Data Heuristics</a><ul><li><a href=#data-augmentation-data>Data augmentation (data)</a></li></ul></li><li><a href=#loss-heuristics>Loss Heuristics</a><ul><li><a href=#weight-decay-loss-regularization>Weight decay (loss, regularization)</a></li></ul></li><li><a href=#training-heuristics>Training Heuristics</a><ul><li><a href=#early-stopping-training-regularization>Early stopping (training, regularization)</a></li></ul></li></ul><ul><li><a href=#metrics>metrics</a><ul><li><a href=#precision-recall-and-f1-score>Precision, recall and F1 Score</a></li></ul></li></ul><ul><li><a href=#collaborative-filtering>Collaborative filtering</a><ul><li><a href=#matrix-factorization>Matrix Factorization</a></li></ul></li><li><a href=#content-based-filtering>Content-based filtering</a></li></ul></nav></div></div><div class=content id=content><p>The objective of this post is to recapitulate the basics of statistics and machine learning following a glossary form.</p><h1 id=basic-maths>Basic Maths</h1><h2 id=eigenvalues-eigenvectors-and-eigendecomposition-singular-value-decomposition>Eigenvalues, Eigenvectors and Eigendecomposition (Singular Value Decomposition)</h2><p>$v$ is an eigenvector and $\lambda$ an eigenvalue for a square matrix $M$ if:</p><p>$Mv=\lambda v$</p><p>$M$ is called <em>diagonalizable</em> if $\exists D$ diagonal, $P$ invertible such a:</p><p>$M=PDP^{-1}$</p><p>In such case, $P$ column vectors are the <em>eigenvectors</em> and $D$ diagonal values are the <em>eigenvalues</em>. This is the <em>eigendecomposition</em> of $M$.</p><p>Some interesting properties:</p><ul><li>All symmetric matrix (i.e. $M=M^T$) is diagonalizable by an orthogonal matrix (i.e. $P^TP=PP^T=I$, and so $P^T=P^{-1}$ and column/row vector norms =1)</li><li>$M^n=PD^nP^{-1}$ and so $M^TM=PD^2P^{-1}$ and $M^{-1}=PD^{-1}P^{-1}$</li></ul><h2 id=sphering-whitening-of-a-matrix>Sphering (Whitening) of a Matrix</h2><p>Let $X$ be a $(p,n)$-dimensional matrix of $n$ samples and $p$ features, we suppose it&rsquo;s centered.</p><p>We are looking for a transformation (a basis) in which the dimensions are uncorrelated and of unit variance. that is, we are looking for a $(p,p)$-matrix $W$ such as</p><p>$Y=WX$</p><p>and</p><p>$Cov(Y):=\frac{1}{n}YY^T=I$</p><p>with $I$ the identity matrix. We have:</p><p>$Cov(X):=\frac{1}{n}XX^T=PDP^{-1}$</p><p>with $P$, $D$ the SVD decomposition of $Cov(X)$.</p><p>So by replacing $Y$ by $WX$, then $XX^T$ by the SVD, we are looking for $W$ such as:</p><p>$WPDP^{-1}W^T=I$</p><p>It is solved by taking $W:=D^{-\frac{1}{2}}P^{-1}$</p><p>It exists and is easy to compute because $D$ is diagonal.</p><p><a href=https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/ target=_blank rel="noopener noreffer">A good reference</a></p><h1 id=descriptive-statistics>Descriptive statistics</h1><h2 id=hypothesis-testing-and-p-value>Hypothesis Testing and P-Value</h2><p>We start with a null hypothesis H0. The alternate hypothesis is H1.</p><p>The <em>P-Value</em> is:</p><p>$P$(Observed Results are consistent with H1 | H0 is true).</p><p>We usually fix a threshold $p$ (e.g. 5%), and we reject the null hypothesis if P-value &lt; $p$.</p><h2 id=confidence-interval>Confidence interval</h2><p>For a given level $\gamma$ (e.g. 95%) and a estimated parameter $\theta$, a <em>confidence interval</em> for $\theta$ is a range $[\hat{\theta}^{min}, \hat{\theta}^{max}]$ such as</p><p>$P(\hat{\theta}^{min} &lt; \theta &lt; \hat{\theta}^{max})=\gamma$</p><p>$\theta$ is deterministic but unknown, the confidence interval is random and observed from an experience. It means that if we redo the experiment 100 times, the confidence interval will contain $\theta$ in average $\gamma$% of the times.</p><p>In Practice, $\theta$ might be the mean of the distribution of a random variable $X$, and we will observe a great number of independent realizations of $X$ and construct a confidence interval around the observed average.</p><h1 id=statistical-learning-principles>Statistical Learning Principles</h1><h2 id=regression-classification-clustering>Regression, classification, clustering</h2><ul><li><strong>Regression</strong>: labels are continuous</li><li><strong>Classification</strong>: labels are categories</li><li><strong>Clustering</strong>: no labels, group data into clusters</li></ul><h2 id=supervised-unsupervised-self-supervised-learning>Supervised, unsupervised, self-supervised learning</h2><ul><li><strong>Supervised</strong>: data is labeled. e.g. <em>regression</em>, <em>classification</em></li><li><strong>Unsupervised</strong> : data is unlabeled, it is used to find patterns in data. e.g <em>clustering</em></li><li><strong>self-supervised</strong>: labels are in the data itself. e.g <em>language modeling</em>.</li></ul><h2 id=loss-function--cost-function>Loss function / Cost function</h2><p>A function (to minimize) that measures how well model predictions match labels. It should be <em>Lipschitz continuous</em> (bounded derivative) for <em>gradient descent</em>.</p><h3 id=mean-square-error>Mean Square Error</h3><p>Usual loss function for regressions. It penalizes heavily big prediction errors.</p><p>$MSE=\frac{1}{n}||Y-\hat{Y}||^2=\frac{1}{n}\sum_{i=1}^n (Y_i-\hat{Y}_i)^2$</p><h3 id=cross-entropy>Cross Entropy</h3><p>Usual loss function for classifications. It&rsquo;s equivalent to maximizing the <em>likelihood</em> of the distribution.</p><p>$ CE=-\frac{1}{n}\sum_{i=1}^n Y_i \cdot \ln{\hat{Y_i}}$</p><p>with $Y_i$ being a one-hot vector for the $i$-th sample with $1$ on the expected class</p><h2 id=training-set-validation-set-test-set>Training set, Validation set, Test set</h2><ul><li><strong>training set</strong>: to train the model. ~75% of the data</li><li><strong>validation set</strong>: to check that you didn&rsquo;t <em>overfit</em> your model to the <em>training set</em>. ~20% of the data</li><li><strong>test set</strong>: to check that you didn&rsquo;t <em>overfit</em> your model hyper-parameters to your <em>validation set</em>. ~5% of the data.</li></ul><h2 id=bias-variance-and-bias-variance-trade-off>Bias, Variance and bias-variance trade-off</h2><p>We take the hypothesis that we can decompose any output $y$ for an input $x$ as:</p><p>$y=f(x)+\epsilon$</p><p>with $f$ being the true function to approximate and $\epsilon$ a random variable (called <em>irreductible error</em>) such as $E[\epsilon]=0$ and $Var(\epsilon)=\sigma^2$</p><p>When computing <em>MSE</em> of the estimator $\hat{f}$, we get that:</p><p>$MSE=Bias(\hat{f})^2+Var(\hat{f})+\sigma^2$</p><p>with:</p><p>$Bias(\hat{f})=E[\hat{f}-f]$</p><p>$Var(\hat{f})=E[(\hat{f}-E[\hat{f}])^2]$</p><p><em>Bias</em> is due to underfitting (lack of power of the algorithm), <em>Variance</em> is due to <em>overfitting</em>.</p><h2 id=overfitting-and-underfitting>Overfitting and underfitting</h2><p>There is <em>overfitting</em> when the <em>loss</em> is a lot better on the <em>training set</em> than on the <em>validation set</em>. It means that the learned function doesn&rsquo;t generalize well to unseen data. Common solutions are <em>regularization</em> techniques, using a simpler model, <em>feature selection</em>, <em>dimensionality reduction</em>, <em>early stopping</em>, <em>dropout</em>&mldr;</p><p>There is <em>underfitting</em> when the <em>bias</em> is high and the <em>loss</em> is too high even on the training set. The common solutions are <em>feature engineering</em> and using a better more powerful model.</p><h1 id=base-algorithms>Base algorithms</h1><h2 id=principal-component-analysis>Principal Component Analysis</h2><p>Standard dimension reduction technique.</p><p>Let $X$ be a $(n,p)$-dimensional matrix of $n$ samples and $p$ features.</p><p>After having centered-reduced $X$ (to scale features), we want to find the sequence $(w_1, w_2&mldr;w_p)$ of $p$-dimensional orthogonal unit vectors (let&rsquo;s call it the $W_p$ orthogonal matrix) such that for any $q&lt;p$, $W_q:=(w_1,..w_q)$ is the $q$-basis that minimizes the MSE of the distance between $X$ and it&rsquo;s projection on the suspace generated by $W_q$.</p><p>We want to minimize:</p><p>$\frac{1}{n}\sum_{i=0}^n |x_i-W_q&lt;x_i,W_q>|^2=\frac{1}{n}\sum_{i=0}^n |x_i-W_q(W_q^Tx_i)|^2$</p><p>We <a href=https://leimao.github.io/article/Principal-Component-Analysis/ target=_blank rel="noopener noreffer">can show</a> that the optimal solution is found by applying <em>SVD</em> to the covariance matrix $\frac{1}{n}X^TX$ and then to choose $W$ as its <em>eigenvectors</em> by decreasing orders of <em>eigenvalue</em>.</p><p>The final reduced matrix is the matrix $XW_q$, it&rsquo;s a $(n,q)$-dimensional matrix.</p><h2 id=least-squares>Least squares</h2><p>In a regression problem, let $Y$ be the vector of the $n$ expected outputs and $X$ the $(n,p)$ input matrix. We suppose that the output can be approximated by a linear combination of the inputs. We are looking for the $p$-dimensional vector $\beta$ to minimize the MSE:</p><p>$||Y-X \beta||^2$</p><p>If we place ourselves in a $n$-dimensional vector space and call $V$ the $p$-dimensional subspace generated by $X$. Minimizing MSE is equivalent to finding $\beta$ such as $X\beta$ is the projection of $Y$ on $V$. That is:</p><p>$&lt;Y-X\beta, Xb>=0, \forall b \in \mathbb{R}^p$</p><p>$(Xb)^T(Y-X\beta)=0, \forall b$</p><p>$b^TX^T(Y-X\beta)=0, \forall b$</p><p>$X^T(Y-X\beta)=0$</p><p>$X^TY=X^TX\beta$</p><p>We conclude that</p><p>$\beta=(X^TX)^{-1}X^TY$</p><h2 id=logistic-regression>Logistic regression</h2><p>In a binary classification problem, let $Y$ be the vector of the $n$ expected outputs, with values $0$, $1$ for respectively the first and second class, and $X$ the $(n,p)$ input matrix. We are looking for the $p$-dimensional vector $\beta$ to minimize the cross-entropy $CE(Y,\hat{Y})$ with:</p><p>$\hat{Y}=\frac{1}{1+\exp{X \beta}}$</p><p>We can show it&rsquo;s a convex optimization problem (the second derivatives of CE with respect to all $\beta_j$ is positive) and can be solved with convex optimization algorithms (Newton&mldr;)</p><h2 id=linearquadratic-discriminant-analysis-ldaqda>Linear/Quadratic Discriminant Analysis (LDA/QDA)</h2><p>LDA is a supervised classification algorithm.</p><p>The hypothesis is:</p><ul><li>Each class $k$ has a multivariate gaussian distribution with density $f_k(x):=P(X=x|C=k)$ with mean $(p)$-vector $\mu_k$ and covariance $(p,p)$-matrix $\Sigma_k$</li><li>$P(C=k)=\pi_k$</li><li><strong>For LDA</strong>: all classes have the same covariance matrix $\Sigma$, only the mean vectors $\mu_k$ are differents</li></ul><p>For a given input vector $x$, we want to find the class $k$ such that:</p><p>$k:=arg max_{k=1..K} P(C=k|X=x)$</p><p>We get from <em>Bayes Theorem</em> that:</p><p>$P(C=k|X=x)=P(X=x|C=k)\frac{P(C=k)}{P(X=x)}=f_k(x)\frac{\pi_k}{\sum_{l=1}^{K}f_l(x)\pi_l}$</p><p>Since we just want to compare them, we can compute $\log (f_k(x)\pi_k)$ for every $k$ and remove factors common to all $k$. This gives us for every $k$ the <em>quadratic discriminant functions</em>:</p><p>$\delta_k(x)=-\frac{1}{2}\log|\Sigma_k|-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)+\log\pi_k$</p><p>To compute this function, we can see that we need to compute:</p><ul><li>The inverse of the covariance matrix</li><li>The determinant $|\Sigma_k|$ of the covariance matrix</li></ul><p>To simplify this computation, we would like to find a basis in which it&rsquo;s enough to compute the distance to the <em>centroid</em> ($\mu_k$) of the distributions</p><p>This is done by <em>Sphering</em> (<em>Whitening</em>) the data. Let:</p><p>$\Sigma_k=U_kD_kU_k^T$</p><p>be the SVD decomposition, we can derive the computation from the expression of $\delta_k(x)$ to see that:</p><p>$\delta_k(x)=-\frac{1}{2}\log|\Sigma_k|-\frac{1}{2}||X^*-\mu_k^*||^2+\log\pi_k$</p><p>with $X^*$ and $\mu_k^*$ be the transform of $X$ and $\mu_k$ in the sphered basis, that is:</p><ul><li>$X^*:=D^{-\frac{1}{2}}U^TX$</li><li>$\mu_k^*:=D^{-\frac{1}{2}}U^T\mu_k$</li></ul><p>To sum it up, the LDA (or QDA) process is:</p><ul><li>Compute $\Sigma$ ($\Sigma_k$) from the dataset</li><li>Compute the SVD decomposition of $\Sigma$ ($\Sigma_k$)</li><li>Compute the determinant and the transformed class centroids $\mu_k^*$</li><li>For a given $x$, compute $x^*$ and its distance to the centroids</li><li>Classify $x$ to the class $k$ that minimizes $\delta_k(x)$</li></ul><h3 id=use-lda-as-a-dimension-reduction-algorithm>Use LDA as a Dimension reduction algorithm</h3><p>Suppose that we want to display the LDA classification in 2 dimensions. We want to find the axis that best split the different classes.</p><p>The usual criteria used is to take the axis that maximizes the variance of the centroids. This is what PCA does. So we do the following PCA decomposition:</p><p>Let $\mu^*:=(\mu_1^*,..\mu_K^*)$ be the $(p,k)$-matrix of the centroids in the transformed bases. We decompose its covariance matrix using PCA:</p><p>$Cov(\mu^*)=U_{\mu}^*D_{\mu}^*U_{\mu}^{*-1}$</p><p>The columns of $U_{\mu}^*$ are the axis we are looking for. We get the coordinates of a vector $x$ by transforming to $x^*$ and taking the scalar products with $u_{\mu,0}^*, u_{\mu,1}^*$&mldr;</p><blockquote><p>NB: In ESL and most blog posts that seems to base their explanation on it, the dimension reduction algorithm uses <a href=https://towardsdatascience.com/pca-whitening-vs-zca-whitening-a-numpy-2d-visual-518b32033edf target=_blank rel="noopener noreffer">ZCA Whitening</a> instead of PCA whitening (that it uses before), this explains the reference to $W^{-\frac{1}{2}}$ everywhere to do the whitening.</p></blockquote><h4 id=good-references>Good references</h4><ul><li><p><a href=https://stats.stackexchange.com/questions/405541/computation-of-lda-in-elements-of-statistical-learning-4-3-2 target=_blank rel="noopener noreffer">computation details of LDA in ESL</a></p></li><li><p><a href=https://yangxiaozhou.github.io/data/2019/10/02/linear-discriminant-analysis.html target=_blank rel="noopener noreffer">Good explanation of dimension reduction</a></p></li><li><p><a href=https://www.stat.cmu.edu/~ryantibs/datamining/lectures/21-clas2.pdf target=_blank rel="noopener noreffer">Another good explanation of LDA not based directly on ESL</a></p></li></ul><h1 id=deep-learning>Deep Learning</h1><h2 id=back-propagation>Back propagation</h2><p>Algorithm used to compute the gradient of the loss function $\nabla L(w)$ with respect to all model parameters $w$.</p><p>It&rsquo;s based on <em>dynamic programming</em> and the <em>chain rule</em></p><h2 id=gradient-descent>Gradient descent</h2><p>Basic version of the optimization algorithms used in Deep Learning. Let $w$ be the vector of all model parameters and $L(w)$ the <em>loss function</em>. We compute $w$ iteratively using:</p><p>$
w = w - \lambda_{lr} * \nabla L(w)
$</p><p>$\lambda_{lr}$ is called the <em>learning rate</em>. It is a trade-off between:</p><ul><li><em>Too high</em>: loss is unstable and diverges</li><li><em>Too low</em>: learning is slow because more step are needed before convergence</li></ul><h2 id=stochastic-batch-gradient-descent>Stochastic (batch) gradient descent</h2><p>In deep learning setup, data size makes naïve GD unpractical. Instead SGD updates $w$ in a small subset called <em>batch</em> of <em>mini-batch</em></p><p>$
w = w - \lambda_{lr} * \nabla L_i(w)
$</p><p>with $L_i$ loss of the $i$-th batch. SGD has also a regularizing effect (by making noisy update step and thus preventing local minima)</p><p>The sample number by batch is called <em>batch size</em>, usually ~64 and is a trade-off between:</p><ul><li><em>Too high</em>: each step is too slow (gradient computation), high memory usage, less regularization</li><li><em>Too low</em>: bad usage of vectorization performance</li></ul><p>NB:</p><ul><li><em>Lerning rate</em> should decrease when <em>batch size</em> decreases (to compensate for more noisy gradients)</li><li>A pass through the whole dataset is called an <em>epoch</em>. data is usually shuffled between epochs to prevent cycles in learning.</li></ul><h2 id=momentum-rmsprop-and-adam>Momentum, RMSProp and Adam</h2><h3 id=momentum>Momentum</h3><p>When computing the gradient, it will be noisy (it changes at each batch) on some dimensions, and more steady in another dimensions.</p><p>Momentum stabilizes gradient (removes noise) by replacing current batch gradient by an exponential moving average.</p><p>$\nabla_{mom}^{n} L = \lambda_{mom}\nabla_{mom}^{n-1}L + (1-\lambda_{mom})\nabla L(w)$</p><p>In practice:</p><ul><li>Momentum factor $\lambda_{mom}$ is usually ~0.9</li><li>We omit the $(1-\lambda_{mom})$ factor by just adjusting the learning rate</li></ul><h3 id=rmsprop-root-mean-square-propagation>RMSProp (Root Mean Square Propagation)</h3><p><em>RMSProp</em> is a way to automatically adapt the learning rate to the current scale of the gradient for each parameter.</p><p>let $g_t^i$ the real gradient at step $t$ for the $i$-th parameter</p><p>At each step and for each parameter:</p><ol><li>We evaluate the exponential moving average of the square of the size of the gradient $\bar{g}_t^{2,i}$. &ldquo;$\bar{~}$&rdquo; for the average, &ldquo;$~^2$&rdquo; for the value homogeneous to a square.</li></ol><p>$\bar{g}_t^{2,i}=\lambda_{rms}\bar{g}_{t-1}^{2,i}+(1-\lambda_{rms})(g_t^i)^2$</p><ol start=2><li>We scale le learning rate</li></ol><p>$\lambda_{lr-rms,t}^i=\lambda_{lr}\frac{1}{\sqrt{\bar{g}_t^{2,i}+\epsilon}}$</p><ol start=3><li>We use this adapted learning rate for the next SGD step</li></ol><p>$
w = w - \lambda_{lr-rms,t} * \nabla L(w)
$</p><h3 id=adam-adaptative-moment-optimization>Adam (Adaptative Moment Optimization)</h3><p>Let&rsquo;s recapitulate:</p><ul><li><em>Momentum</em> smooth the gradient to reduce noise</li><li><em>RMSProp</em> adapt the learning rate to the average scale of the gradient</li></ul><p><em>Adam</em> combines both techniques.</p><p>NB: In practice, <em>Adam</em> often converges faster than pure momentum, but model generalization may be worst.</p><h2 id=hyper-parameters>Hyper parameters</h2><p>All parameters externals (not learned) to the model (<em>batch size</em>, <em>learning rate</em>&mldr;). They are usually found by <em>grid search</em> or <em>random search</em>.</p><h2 id=common-layers>Common Layers</h2><h3 id=sigmoid>Sigmoid</h3><p>$y=\frac{1}{1+e^{-x}}$</p><p>Usual output layer for binary classification. Rescale $]-\infty, +\infty[$ to $]0,1[$ so it&rsquo;s interpretable as a probability.</p><h3 id=softmax>Softmax</h3><p>$y_i=\frac{e^{x_i}}{\sum_{i=1}^{n}e^{x_i}}$</p><p>Usual output layer for classification with $n$ classes. Sum to $1$ so it&rsquo;s interpretable as probabilities.</p><h3 id=relu>ReLU</h3><p>$y=max(0,x)$</p><p>Usual activation function of intermediate dense layers, it introduce a non linearity in the network so that models can learn complex representations.</p><p>There is a lot of varation of it (GELU, Leaky ReLu&mldr;)</p><h2 id=model-heuristics>Model Heuristics</h2><h3 id=batch-normalization>Batch normalization</h3><p>This technique improve convergence and generalization significantly, but we <a href=https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338#3fb5 target=_blank rel="noopener noreffer">don&rsquo;t know really why</a>.</p><p>It consists in renormalizing (center-reduce) outputs between two layers batch by batch, and rescaling them up based on new learned parameters.</p><p>With:</p><ul><li>$d$ dimension of the normalized layer output</li><li>$m$ batch size</li><li>$X$ $(d,m)$-matrix ou the previous layer output</li><li>$X_{bn}$ output of the batch norm layer</li><li>$b$ $d$-vector of learned bias</li><li>$s$ $d$-vector of learned scaling</li><li>$\bar{X}$ $d$-vector of average of $X$ along the batch axis</li><li>$\sigma^2$ $d$-vector of variance of $X$ along the batch axis</li></ul><p>$X_{bn}= s*\frac{X-\bar{X}}{\sqrt{\sigma^2+\epsilon}}+b
$</p><h3 id=dropout-model-regularization>Dropout (model, regularization)</h3><p>Common regularization technique. It consists in adding a layer that will <em>drop</em> (i.e. set to $0$) intermediate dimensions randomly at a given probability $P$.</p><p>At training time, we rescale (i.e. divide by $1-P$) the remaining values to that expectation remains the same.</p><p>At test time, dropout layer is ignored.</p><h3 id=residual-connection>Residual connection</h3><p>Deep architectures suffer from a problem called <em>Vanishing gradient</em>: gradients become close to zero far from output layers.</p><p>Residual connections &ldquo;skip&rdquo; layers so every layers are closer to output.</p><p>For a given layer fonction $F(x)$, the residual connection is:</p><p>$y=F(x)+x$</p><h2 id=data-heuristics>Data Heuristics</h2><h3 id=data-augmentation-data>Data augmentation (data)</h3><p>Increasing the amount of training data by adding modified copies of raw data. e.j: various rotation of images.</p><h2 id=loss-heuristics>Loss Heuristics</h2><h3 id=weight-decay-loss-regularization>Weight decay (loss, regularization)</h3><p>It has been proven that models generalize better when model parameters stay close to zero.</p><p>As for <em>Ridge regression</em>, we modify loss function to penalize the $L2$-norm of the vector $w$ of all models parameters, the penalization factor $\lambda_{wd}$ is called <em>weight decay</em></p><p>$L_{wd}(w)=L(w)+\lambda_{wd}||w||^2$</p><p>We can directly plug it in the <em>Gradient descent</em> formula using</p><p>$\nabla L_{wd} = \nabla L + 2\lambda_{wd}w$</p><h2 id=training-heuristics>Training Heuristics</h2><h3 id=early-stopping-training-regularization>Early stopping (training, regularization)</h3><p>On successive <em>epochs</em> of training, we sometime empirically see that past a certain point, <em>validation error</em> starts to increases while <em>training error</em> continue to decreases. Model is overfitting. <em>Early stopping</em> consists in stopping earning at this point (before training error stabilizes itself)</p><h1 id=natural-language-processing>Natural Language Processing</h1><h2 id=metrics>metrics</h2><h3 id=precision-recall-and-f1-score>Precision, recall and F1 Score</h3><p>used for binary classification tasks like document retrieval.</p><ul><li><p><strong>Precision</strong>: $ \frac{true\ positive}{true\ positive + false\ positive}$. 1 when only relevant items are found</p></li><li><p><strong>Recall</strong>: $ \frac{true\ positive}{true\ positive + false\ negative}$. 1 when all relevant items are found</p></li><li><p><strong>F1 Score</strong>: harmonic mean of recall and prevision (harmonic penalize imbalanced models between recall and precision): $\frac{2}{\frac{1}{recall}+\frac{1}{precision}}$</p></li></ul><h1 id=recommender-systems>Recommender systems</h1><p>The goal is to recommend the most relevant items to users. Most of the time, &ldquo;recomending&rdquo; an item means guessing the ratings that a user would give to any item given its rating for other items. One example is movie recommendations.</p><h2 id=collaborative-filtering>Collaborative filtering</h2><p>This applies well when:</p><ul><li>We don&rsquo;t have natural feature vectors for items or users, and so we have to infer them from existing ratings (using some distance measure between users and between items)</li><li>We have a way to solve the <em>cold-start</em> problems (getting initial ratings for a new item or a new user).</li></ul><h3 id=matrix-factorization>Matrix Factorization</h3><p>Let&rsquo;s model the ratings as a $(nbUsers,nbItems)$ matrix $R$ containing the ratings. This matrix is <em>sparse</em> and we want to fill in the holes.</p><p>We suppose that items and users can be modelized features called <em>latent factors</em>. Those features are the model parameters.</p><p>We compute a measure of the distance between a user and an item using a dot-product. We then use a sigmoid function to scale the rating as needed.</p><p>because a few lines of code are worth a thousand words, here is the model:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MatrixFactorization</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>nb_users</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>nb_items</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>nb_factors</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>MatrixFactorization</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>user_embeddings</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>nb_users</span><span class=p>,</span> <span class=n>nb_factors</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># some users might have a tendency to give only high or low ratings, this bias deal with it</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>user_bias</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>nb_users</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>item_embeddings</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>nb_items</span><span class=p>,</span> <span class=n>nb_factors</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># same as for users</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>item_bias</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>nb_factors</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># user_item_pairs_batch is a (batch_size, 2) tensor</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>user_item_pairs_batch</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_user_idx_s</span> <span class=o>=</span> <span class=n>user_item_pairs_batch</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_item_idx_s</span> <span class=o>=</span> <span class=n>user_item_pairs_batch</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>batch_user_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>user_embeddings</span><span class=p>[</span><span class=n>batch_user_idx_s</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_item_embeddings</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>item_embeddings</span><span class=p>[</span><span class=n>batch_item_idx_s</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># raw ratings by batch entry before bias and sigmoid post-processing</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_raw_ratings</span> <span class=o>=</span> <span class=p>(</span><span class=n>batch_user_embeddings</span><span class=o>*</span><span class=n>batch_item_embeddings</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>raw_ratings_biased</span> <span class=o>=</span> <span class=n>batch_raw_ratings</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>user_bias</span><span class=p>[</span><span class=n>batch_user_idx_s</span><span class=p>]</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>movie_bias</span><span class=p>[</span><span class=n>batch_item_idx_s</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Scale ratings to [0,1]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>raw_ratings_biased</span><span class=p>)</span>
</span></span></code></pre></div><p>The loss function is usually just MSE.</p><p>Of course, from this formulation, we can move to <em>Deep Learning Factorization</em> by using a more complex model than just a sigmoid of the embeddings product.</p><h2 id=content-based-filtering>Content-based filtering</h2></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-12-12</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/python-cheat-sheet/ class=prev rel=prev title="Python cheatsheets for machine learning"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Python cheatsheets for machine learning</a></div></div></article></div></main></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>