<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>How to adapt the Transformer architecture to generate Sets - Nicolas Lecrique</title><meta name=Description content><meta property="og:title" content="How to adapt the Transformer architecture to generate Sets"><meta property="og:description" content="Generation is one of the most complex tasks in Machine Learning. There is a huge literature about images and text. But how do you generate sets?"><meta property="og:type" content="article"><meta property="og:url" content="https://nicolaslecrique.github.io/posts/adapt-transformer-architecture-to-set-generation/"><meta property="og:image" content="https://nicolaslecrique.github.io/posts/adapt-transformer-architecture-to-set-generation/featured-image.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-07-15T10:48:07+02:00"><meta property="article:modified_time" content="2022-07-15T10:48:07+02:00"><meta property="og:site_name" content="Nicolas Lecrique"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nicolaslecrique.github.io/posts/adapt-transformer-architecture-to-set-generation/featured-image.png"><meta name=twitter:title content="How to adapt the Transformer architecture to generate Sets"><meta name=twitter:description content="Generation is one of the most complex tasks in Machine Learning. There is a huge literature about images and text. But how do you generate sets?"><meta name=application-name content="Nicolas Lecrique"><meta name=apple-mobile-web-app-title content="Nicolas Lecrique"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://nicolaslecrique.github.io/posts/adapt-transformer-architecture-to-set-generation/><link rel=prev href=https://nicolaslecrique.github.io/posts/croissance-verte/><link rel=next href=https://nicolaslecrique.github.io/posts/statistics-machine-learning-basics/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"How to adapt the Transformer architecture to generate Sets","inLanguage":"en-us","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/nicolaslecrique.github.io\/posts\/adapt-transformer-architecture-to-set-generation\/"},"image":[{"@type":"ImageObject","url":"https:\/\/nicolaslecrique.github.io\/posts\/adapt-transformer-architecture-to-set-generation\/featured-image.png","width":450,"height":400}],"genre":"posts","wordcount":1679,"url":"https:\/\/nicolaslecrique.github.io\/posts\/adapt-transformer-architecture-to-set-generation\/","datePublished":"2022-07-15T10:48:07+02:00","dateModified":"2022-07-15T10:48:07+02:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Nicolas Lecrique"},"description":""}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.css integrity=sha384-Cqd8ihRLum0CCg8rz0hYKPoLZ3uw+gES2rXQXycqnL5pgVQIflxAUDS7ZSjITLb5 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/katex.min.js integrity=sha384-1Or6BdeNQb0ezrmtGeqQHFpppNd7a/gw29xeiSikBbsb44xu3uAo8c7FwbF5jhbd crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.2/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Nicolas Lecrique">Nicolas Lecrique</a></div><div class=menu><div class=menu-inner><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Nicolas Lecrique">Nicolas Lecrique</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">How to adapt the Transformer architecture to generate Sets</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Nicolas Lecrique</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2022-07-15>2022-07-15</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1679 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;8 minutes&nbsp;</div></div><div class=featured-image><img class=lazyload src=/svg/loading.min.svg data-src=/posts/adapt-transformer-architecture-to-set-generation/featured-image.png data-srcset="/posts/adapt-transformer-architecture-to-set-generation/featured-image.png, /posts/adapt-transformer-architecture-to-set-generation/featured-image.png 1.5x, /posts/adapt-transformer-architecture-to-set-generation/featured-image.png 2x" data-sizes=auto alt=/posts/adapt-transformer-architecture-to-set-generation/featured-image.png title=/posts/adapt-transformer-architecture-to-set-generation/featured-image.png></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#the-problem-with-set-generation>The problem with set generation?</a></li><li><a href=#potential-architectures-vae-gan-and-ar>Potential architectures: VAE, GAN and AR</a></li><li><a href=#the-transformer-architecture>The Transformer architecture</a></li><li><a href=#baseline-model-take-the-transformer-as-is>Baseline model: take the transformer as is</a><ul><li><a href=#transformer-model>Transformer model</a></li><li><a href=#positional-encoding>Positional encoding</a></li><li><a href=#loss-function>Loss function</a></li></ul></li><li><a href=#adaptation-1-get-rid-of-the-positional-encoding>Adaptation 1: Get rid of the positional encoding</a></li><li><a href=#adaptation-2-use-the-soft-cross-entropy-loss-function>Adaptation 2: Use the Soft cross-entropy loss function</a><ul><li><a href=#soft-cross-entropy>Soft cross-entropy</a></li><li><a href=#probabilistic-target-vectors>Probabilistic target vectors</a></li><li><a href=#pytorch-limitation>Pytorch limitation</a></li></ul></li><li><a href=#adaptation-2-bis-recode-the-soft-cross-entropy-loss-function>Adaptation 2 (Bis): Recode the Soft cross-entropy loss function</a><ul><li><a href=#soft-cross-entropy-from-scratch>Soft cross-entropy from scratch</a></li><li><a href=#add-label-smoothing-back-into-our-new-loss>Add label smoothing back into our new loss</a></li><li><a href=#comparison-to-the-standard-loss-function>Comparison to the standard loss function</a></li></ul></li><li><a href=#lets-sum-it-up>Let&rsquo;s sum it up</a></li><li><a href=#source-code>Source code</a></li></ul></nav></div></div><div class=content id=content><p>Generation is one of the most complex tasks in Machine Learning. There is a huge literature about images and text. But how do you generate sets?</p><h2 id=the-problem-with-set-generation>The problem with set generation?</h2><p>Set generation is the ability to generate unordered subsets of a much bigger set following an implicit probability distribution.</p><p>The keyword here is <em>unordered</em></p><ul><li><p>An image is not a set of pixels, because there is a notion of <em>geometry</em> between them.</p></li><li><p>A text is not a set of words, it&rsquo;s a sequence, because there is a notion of <em>order</em> between words.</p></li></ul><p>An example of a set is the list of ingredients in a dish. This is actually the use case I had to make a generative model for.</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/adapt-transformer-architecture-to-set-generation/subsets_from_total_set.png data-srcset="/posts/adapt-transformer-architecture-to-set-generation/subsets_from_total_set.png, /posts/adapt-transformer-architecture-to-set-generation/subsets_from_total_set.png 1.5x, /posts/adapt-transformer-architecture-to-set-generation/subsets_from_total_set.png 2x" data-sizes=auto alt=/posts/adapt-transformer-architecture-to-set-generation/subsets_from_total_set.png title="generate subsets from total set" width=719 height=840></p><p>This absence of natural order or geometry makes it difficult to directly use the models used for texts and images</p><h2 id=potential-architectures-vae-gan-and-ar>Potential architectures: VAE, GAN and AR</h2><p>There are 3 broad classes of generative models commonly used:</p><ul><li><a href=https://en.wikipedia.org/wiki/Variational_autoencoder target=_blank rel="noopener noreffer">Variational Autoencoders</a></li><li><a href=https://en.wikipedia.org/wiki/Generative_adversarial_network target=_blank rel="noopener noreffer">Generative Adversarial Network</a></li><li><a href=https://en.wikipedia.org/wiki/Autoregressive_model target=_blank rel="noopener noreffer">Autoregressive Models</a></li></ul><p>The first two are mainly used for image generation while the transformer architecture is mainly used for text generation.</p><p>In theory, all 3 could be used for sets generation:</p><ul><li>VAEs generate usual feature vectors. A subset can be represented as a vector of zeros and ones where each index represents an element of the set.</li><li>GANs is more of a strategy to create a generator model than a specific model. As such, it can be adapted to any use case.</li><li>ARs can be used to generate sets by just ignoring the order of the generated sequences.</li></ul><p>In practice,</p><ul><li>VAEs are sensitive to a phenomenon called <a href=https://datascience.stackexchange.com/questions/48962/what-is-posterior-collapse-phenomenon target=_blank rel="noopener noreffer">posterior collapse</a> that makes them unsuitable for sets generation. For example, when generating a set of ingredients for a pie, it will select a bit of puff, shortcrust, and shortbread pastry instead of choosing one.</li><li>GANs demand a lot of tuning and a big dataset to work. It makes them impractical if you don&rsquo;t have infinite resources.</li></ul><p>This makes Autoregressive Models the most promising ones for sets generation. Among them, the Transformer architecture is by far the most efficient one and completely replaced previous models like RNNs.</p><h2 id=the-transformer-architecture>The Transformer architecture</h2><p>If you need an introduction to the Transformer architecture in the context of sequence generation, the best explanation I know is <a href=https://jalammar.github.io/illustrated-gpt2/ target=_blank rel="noopener noreffer">Here</a>, so I won&rsquo;t detail it here. Let&rsquo;s just recapitulate the different layers to see what we need to change.</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/adapt-transformer-architecture-to-set-generation/Transformer_generator.png data-srcset="/posts/adapt-transformer-architecture-to-set-generation/Transformer_generator.png, /posts/adapt-transformer-architecture-to-set-generation/Transformer_generator.png 1.5x, /posts/adapt-transformer-architecture-to-set-generation/Transformer_generator.png 2x" data-sizes=auto alt=/posts/adapt-transformer-architecture-to-set-generation/Transformer_generator.png title="transformer generator" width=800 height=2000></p><h2 id=baseline-model-take-the-transformer-as-is>Baseline model: take the transformer as is</h2><p>To generate sets, nothing prevents us to reuse the previous architecture as is, using our set elements as the vocabulary and generating sequences. We can then just ignore the order to get a set.</p><p>Below is the Pytorch source code of the transformer generator split in 3 parts:</p><ul><li>The heart of the Transformer model</li><li>The <em>Positional encoding</em> Layer, specific to the Transformer architecture</li><li>The Cross-entropy loss function adapted to Autoregressive Models</li></ul><h3 id=transformer-model>Transformer model</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span> <span class=k>as</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.nn</span> <span class=kn>import</span> <span class=n>TransformerEncoder</span><span class=p>,</span> <span class=n>TransformerEncoderLayer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># This is loosely inspired by the code available here:</span>
</span></span><span class=line><span class=cl><span class=c1># https://pytorch.org/tutorials/beginner/transformer_tutorial.html</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>idx_to_token</span><span class=p>:</span> <span class=p>[</span><span class=nb>str</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                 <span class=n>embedding_dim_between_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>nb_attention_heads</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>hidden_dim_feed_forward_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>nb_encoder_layers</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>padding_idx</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>max_sentence_size</span><span class=p>:</span> <span class=nb>int</span><span class=o>=</span><span class=mi>50</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>TransformerModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>src_mask_by_sequence_size</span> <span class=o>=</span> <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>_generate_square_subsequent_mask</span><span class=p>(</span><span class=n>len_sequence</span><span class=p>)</span> <span class=k>for</span> <span class=n>len_sequence</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_sentence_size</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>vocab_to_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>idx_to_token</span><span class=p>),</span> <span class=n>embedding_dim_between_layers</span><span class=p>,</span> <span class=n>padding_idx</span><span class=o>=</span><span class=n>padding_idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoder</span> <span class=o>=</span> <span class=n>PositionalEncoding</span><span class=p>(</span><span class=n>embedding_dim_between_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>encoder_layers</span> <span class=o>=</span> <span class=n>TransformerEncoderLayer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>embedding_dim_between_layers</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>nb_attention_heads</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>hidden_dim_feed_forward_layers</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>dropout</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>transformer_encoder</span> <span class=o>=</span> <span class=n>TransformerEncoder</span><span class=p>(</span><span class=n>encoder_layers</span><span class=p>,</span> <span class=n>nb_encoder_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding_to_vocab</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embedding_dim_between_layers</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>idx_to_token</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding_dim_between_layers</span> <span class=o>=</span> <span class=n>embedding_dim_between_layers</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>idx_to_token</span> <span class=o>=</span> <span class=n>idx_to_token</span>  <span class=c1># so it&#39;s serialized with model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_init_weights</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Generate a mask for the TransformerEncoderLayer</span>
</span></span><span class=line><span class=cl>    <span class=c1># it allows the generator to use previous but not following tokens to generate the current one.</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_generate_square_subsequent_mask</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>len_sequence</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span> <span class=o>=</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>triu</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>len_sequence</span><span class=p>,</span> <span class=n>len_sequence</span><span class=p>))</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mask</span> <span class=o>=</span> <span class=n>mask</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;-inf&#39;</span><span class=p>))</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>1</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=mf>0.0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>mask</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_init_weights</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>initrange</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>vocab_to_embedding</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>uniform_</span><span class=p>(</span><span class=o>-</span><span class=n>initrange</span><span class=p>,</span> <span class=n>initrange</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding_to_vocab</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding_to_vocab</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>uniform_</span><span class=p>(</span><span class=o>-</span><span class=n>initrange</span><span class=p>,</span> <span class=n>initrange</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># take a tensor of dimensions [sequence_size, batch_size]</span>
</span></span><span class=line><span class=cl>    <span class=c1># sequences is a [sequence_size, batch_size] tensor. Each entry is a token index</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>sequences</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 1. We transform each token to its corresponding embedding.</span>
</span></span><span class=line><span class=cl>        <span class=c1># We get a tensor of dimensions [sequence_size, batch_size, embedding_size]</span>
</span></span><span class=line><span class=cl>        <span class=c1># See http://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder for the normalization factor</span>
</span></span><span class=line><span class=cl>        <span class=n>sequences</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>vocab_to_embedding</span><span class=p>(</span><span class=n>sequences</span><span class=p>)</span> <span class=o>*</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embedding_dim_between_layers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2. We add positional encoding to the embeddings</span>
</span></span><span class=line><span class=cl>        <span class=n>sequences</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoder</span><span class=p>(</span><span class=n>sequences</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3. We pass the sequence through the encoder layers</span>
</span></span><span class=line><span class=cl>        <span class=c1># We get a tensor of dimension [sequence_size, batch_size, embedding_dim_between_layers]</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transformer_encoder</span><span class=p>(</span><span class=n>sequences</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>src_mask_by_sequence_size</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>sequences</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 4. We convert output embeddings to vocabulary size vectors that can be processed by the softmax layer</span>
</span></span><span class=line><span class=cl>        <span class=c1># We get a tensor of dimension [sequence_size, batch_size, vocabulary_size]</span>
</span></span><span class=line><span class=cl>        <span class=c1># the softmax layer is inside the loss function (as it does not contain learnable parameters)</span>
</span></span><span class=line><span class=cl>        <span class=n>vocab</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding_to_vocab</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>vocab</span>
</span></span></code></pre></div><h3 id=positional-encoding>Positional encoding</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PositionalEncoding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>max_len</span><span class=o>=</span><span class=mi>5000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>PositionalEncoding</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>pe</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>max_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>position</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>max_len</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>div_term</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span> <span class=o>*</span> <span class=p>(</span><span class=o>-</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mf>10000.0</span><span class=p>)</span> <span class=o>/</span> <span class=n>d_model</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span> <span class=o>=</span> <span class=n>pe</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s1>&#39;pe&#39;</span><span class=p>,</span> <span class=n>pe</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>pe</span><span class=p>[:</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><h3 id=loss-function>Loss function</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>CrossEntropyTransformerLoss</span><span class=p>(</span><span class=n>TransformerLoss</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>nb_classes</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>pad_idx</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>eos_idx</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>label_smoothing_coeff</span><span class=o>=</span><span class=mf>0.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>TransformerLoss</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>nb_classes</span> <span class=o>=</span> <span class=n>nb_classes</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>loss</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>(</span><span class=n>label_smoothing</span><span class=o>=</span><span class=n>label_smoothing_coeff</span><span class=p>,</span> <span class=n>ignore_index</span><span class=o>=</span><span class=n>pad_idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># input: Tensor [sequence_size, batch_size, vocab_size] of linear output to give to softmax</span>
</span></span><span class=line><span class=cl>    <span class=c1># target: Tensor [sequence_size, batch_size] of token indexes</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>result</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>TransformerLossResult</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>flatten_result</span> <span class=o>=</span> <span class=n>result</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nb_classes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>flatten_target</span> <span class=o>=</span> <span class=n>target</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss_result</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss</span><span class=p>(</span><span class=n>flatten_result</span><span class=p>,</span> <span class=n>flatten_target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>loss_result</span>
</span></span></code></pre></div><h2 id=adaptation-1-get-rid-of-the-positional-encoding>Adaptation 1: Get rid of the positional encoding</h2><p>The positional encoding is here to give information about the relative position of the previous tokens to generate the next one.</p><p>Here all we care about is the set of previous tokens, irrelative to their order. Because positional encoding is just an addition to the embedding vectors, let&rsquo;s just remove the following line in the <code>TransformerModel</code> forward function.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sequences</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_encoder</span><span class=p>(</span><span class=n>sequences</span><span class=p>)</span>
</span></span></code></pre></div><p>To be honest, it didn&rsquo;t improve significantly the loss on the test set in my case, but it&rsquo;s always good to make the code simpler if complexity doesn&rsquo;t bring more performance.</p><h2 id=adaptation-2-use-the-soft-cross-entropy-loss-function>Adaptation 2: Use the Soft cross-entropy loss function</h2><p>This one is a bit trickier. At training time, the standard loss function is the cross-entropy between the output probability distribution of the model and the one-hot vector of the correct next token.</p><p>If we remove any notion of order, the next correct token is any token belonging to the set and not yet generated.</p><p>Thankfully, Pytorch <code>CrossEntropyLoss</code> class can calculate the cross-entropy between two distributions. We can then set as the target a vector where the probability is shared between all not yet generated elements of the set.</p><h3 id=soft-cross-entropy>Soft cross-entropy</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>SoftCrossEntropyLossTransformerLoss</span><span class=p>(</span><span class=n>TransformerLoss</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>nb_classes</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>pad_idx</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>eos_idx</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>label_smoothing_coeff</span><span class=o>=</span><span class=mf>0.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>TransformerLoss</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>nb_classes</span> <span class=o>=</span> <span class=n>nb_classes</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pad_idx</span> <span class=o>=</span> <span class=n>pad_idx</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eos_idx</span> <span class=o>=</span> <span class=n>eos_idx</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>loss</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>(</span><span class=n>label_smoothing</span><span class=o>=</span><span class=n>label_smoothing_coeff</span><span class=p>,</span> <span class=n>ignore_index</span><span class=o>=</span><span class=n>pad_idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># result: Tensor [sequence_size, batch_size, vocab_size] of linear output to give to softmax</span>
</span></span><span class=line><span class=cl>    <span class=c1># target: Tensor [sequence_size, batch_size] of token indexes</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>result</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>TransformerLossResult</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># I) ---- PREPARE TARGET ----</span>
</span></span><span class=line><span class=cl>        <span class=n>target_proba</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_compute_unordered_probabilistic_target</span><span class=p>(</span><span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># II) ---- COMPUTE LOSS ----</span>
</span></span><span class=line><span class=cl>        <span class=c1># we flatten tensors on softmax dimension</span>
</span></span><span class=line><span class=cl>        <span class=n>flatten_result</span> <span class=o>=</span> <span class=n>result</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nb_classes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>flatten_target</span> <span class=o>=</span> <span class=n>target_proba</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nb_classes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss_result</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss</span><span class=p>(</span><span class=n>flatten_result</span><span class=p>,</span> <span class=n>flatten_target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>loss_result</span>
</span></span></code></pre></div><h3 id=probabilistic-target-vectors>Probabilistic target vectors</h3><p>With the following function preparing target vectors</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_compute_unordered_probabilistic_target</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>target</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 1) [sequence_size, batch_size] of indexes to [sequence_size, batch_size, vocabulary_ntokens] one_hot</span>
</span></span><span class=line><span class=cl>        <span class=n>target_one_hot</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>one_hot</span><span class=p>(</span><span class=n>target</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>nb_classes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 2) valid next token is any token in the subsequent sequence except pad and eos</span>
</span></span><span class=line><span class=cl>        <span class=n>seq_length</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i_seq_idx</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>seq_length</span> <span class=o>-</span> <span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=c1># elt[i] = elt[i] + elt[i+1]</span>
</span></span><span class=line><span class=cl>            <span class=n>to_paste</span> <span class=o>=</span> <span class=n>target_one_hot</span><span class=p>[</span><span class=n>i_seq_idx</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=p>:,</span> <span class=p>:]</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>to_paste</span><span class=p>[:,</span> <span class=bp>self</span><span class=o>.</span><span class=n>pad_idx</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>            <span class=n>to_paste</span><span class=p>[:,</span> <span class=bp>self</span><span class=o>.</span><span class=n>eos_idx</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>target_one_hot</span><span class=p>[</span><span class=n>i_seq_idx</span><span class=p>,</span> <span class=p>:,</span> <span class=p>:]</span><span class=o>.</span><span class=n>add_</span><span class=p>(</span><span class=n>to_paste</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 3) if there is several possible next token, give each one equals probability</span>
</span></span><span class=line><span class=cl>        <span class=n>nb_targets</span> <span class=o>=</span> <span class=n>target_one_hot</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>  <span class=c1># keepdims to be broadcastable</span>
</span></span><span class=line><span class=cl>        <span class=n>target_proba</span> <span class=o>=</span> <span class=n>target_one_hot</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>  <span class=c1># cast int to float</span>
</span></span><span class=line><span class=cl>        <span class=n>target_proba</span><span class=o>.</span><span class=n>div_</span><span class=p>(</span><span class=n>nb_targets</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>target_proba</span>
</span></span></code></pre></div><h3 id=pytorch-limitation>Pytorch limitation</h3><p>This should work&mldr;except we are met with the following error:</p><blockquote><p>RuntimeError: ignore_index is not supported for floating point target</p></blockquote><p>Pytorch is telling us that we either use a probabilistic target or the <code>ignore_index</code> parameter of the loss function to not compute loss on padded tokens. This is explained <a href=https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html target=_blank rel="noopener noreffer">here</a></p><h2 id=adaptation-2-bis-recode-the-soft-cross-entropy-loss-function>Adaptation 2 (Bis): Recode the Soft cross-entropy loss function</h2><p>Since Pytorch cannot combine <code>ignore_index</code> and a probabilistic target, let&rsquo;s do it ourselves.</p><h3 id=soft-cross-entropy-from-scratch>Soft cross-entropy from scratch</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SoftCrossEntropyTransformerLossFromScratch</span><span class=p>(</span><span class=n>TransformerLoss</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>nb_classes</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>pad_idx</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>eos_idx</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>label_smoothing_coeff</span><span class=o>=</span><span class=mf>0.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>SoftCrossEntropyTransformerLossFromScratch</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>nb_classes</span> <span class=o>=</span> <span class=n>nb_classes</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>label_smoothing_coeff</span> <span class=o>=</span> <span class=n>label_smoothing_coeff</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pad_idx</span> <span class=o>=</span> <span class=n>pad_idx</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eos_idx</span> <span class=o>=</span> <span class=n>eos_idx</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># input: Tensor [sequence_size, batch_size, vocab_size] of linear output to give to softmax</span>
</span></span><span class=line><span class=cl>    <span class=c1># target: Tensor [sequence_size, batch_size] of token indexes</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>output</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>target</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>TransformerLossResult</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>target_proba_smoothed</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_compute_unordered_probabilistic_target</span><span class=p>(</span><span class=n>target</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># II) ---- COMPUTE LOSS ----</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># we flatten tensors on softmax dimension</span>
</span></span><span class=line><span class=cl>        <span class=n>target_proba_smoothed_flattened</span> <span class=o>=</span> <span class=n>target_proba_smoothed</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nb_classes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>target_flattened</span> <span class=o>=</span> <span class=n>target</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output_flattened</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nb_classes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># compute log_softmax</span>
</span></span><span class=line><span class=cl>        <span class=n>log_prb_output</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>log_softmax</span><span class=p>(</span><span class=n>output_flattened</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># compute cross entropy by softmax</span>
</span></span><span class=line><span class=cl>        <span class=n>loss_by_softmax</span> <span class=o>=</span> <span class=o>-</span><span class=p>(</span><span class=n>target_proba_smoothed_flattened</span> <span class=o>*</span> <span class=n>log_prb_output</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># restrict loss computation to non padded elements</span>
</span></span><span class=line><span class=cl>        <span class=n>non_pad_mask</span> <span class=o>=</span> <span class=n>target_flattened</span><span class=o>.</span><span class=n>ne</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>pad_idx</span><span class=p>)</span>  <span class=c1># vector of True where target is not pad_idx</span>
</span></span><span class=line><span class=cl>        <span class=n>loss_reduced_to_not_pad_index</span> <span class=o>=</span> <span class=n>loss_by_softmax</span><span class=o>.</span><span class=n>masked_select</span><span class=p>(</span><span class=n>non_pad_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>loss_reduced_to_not_pad_index</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span></code></pre></div><h3 id=add-label-smoothing-back-into-our-new-loss>Add label smoothing back into our new loss</h3><p>We are almost done, but our new loss implementation doesn&rsquo;t manage <a href=https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06 target=_blank rel="noopener noreffer">label smoothing</a>, a regularization technique used in generator models to improve generation.</p><p>Let&rsquo;s call this function at the end of the <code>_compute_unordered_probabilistic_target</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_apply_label_smoothing_to_target</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>nb_targets</span><span class=p>,</span> <span class=n>target_proba</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># https://arxiv.org/pdf/1512.00567.pdf, https://arxiv.org/abs/1906.02629</span>
</span></span><span class=line><span class=cl>        <span class=n>nb_targets_smoothing</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>nb_classes</span> <span class=o>-</span> <span class=n>nb_targets</span>
</span></span><span class=line><span class=cl>        <span class=n>smoothing_value</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>label_smoothing_coeff</span> <span class=o>/</span> <span class=n>nb_targets_smoothing</span>
</span></span><span class=line><span class=cl>        <span class=n>target_proba</span><span class=o>.</span><span class=n>mul_</span><span class=p>(</span><span class=mf>1.0</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>label_smoothing_coeff</span><span class=p>)</span>  <span class=c1># apply smoothing</span>
</span></span><span class=line><span class=cl>        <span class=n>target_proba_smoothed</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>target_proba</span><span class=p>,</span> <span class=n>smoothing_value</span><span class=p>)</span>  <span class=c1># use broadcasting</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>target_proba_smoothed</span>
</span></span></code></pre></div><h3 id=comparison-to-the-standard-loss-function>Comparison to the standard loss function</h3><p>Here we should check if the generator trained with this loss function is better than the previous one. This is not obvious because the loss function being modified, the resulting loss value on the test set is not directly comparable.</p><p>In my case, It seems better (it generates more variety), but you should check in your use case.</p><h2 id=lets-sum-it-up>Let&rsquo;s sum it up</h2><p>To adapt the transformer to the generation of sets, we made two modifications:</p><ul><li>On the Network architecture, we removed the positional encoding</li><li>On the loss function, we replaced the standard cross-entropy with a custom soft cross-entropy.</li></ul><h2 id=source-code>Source code</h2><p>The source code for this project is available <a href=https://github.com/nicolaslecrique/set_generator_transformer target=_blank rel="noopener noreffer">here</a></p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-07-15</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/croissance-verte/ class=prev rel=prev title="Une croissance verte est-elle crdible? Calculons"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Une croissance verte est-elle crdible? Calculons</a>
<a href=/posts/statistics-machine-learning-basics/ class=next rel=next title="Statistics and Machine Learning, the basics">Statistics and Machine Learning, the basics<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>